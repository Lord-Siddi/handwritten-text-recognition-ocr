{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End Handwritten Text Recognition\n",
        "\n",
        "This notebook demonstrates the complete OCR pipeline that converts handwritten document images into digital text using a CNNâ€“BiLSTMâ€“CTC model with optional language model decoding.\n"
      ],
      "metadata": {
        "id": "MN3Xy4tji5DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import joblib\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "from pyctcdecode import build_ctcdecoder\n",
        "import pyctcdecode.decoder as decoder_module\n",
        "import pyctcdecode.language_model as language_model_module\n",
        "import kenlm\n"
      ],
      "metadata": {
        "id": "-5LJFrzci8W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = \"models/htr_model\"\n",
        "KENLM_BINARY = \"models/iam_lm_5gram.binary\"\n",
        "\n",
        "model = keras.models.load_model(os.path.join(MODEL_DIR, \"htr_model.keras\"), compile=False)\n",
        "vocab_list = joblib.load(os.path.join(MODEL_DIR, \"vocab_list.pkl\"))\n"
      ],
      "metadata": {
        "id": "0UJnEOrri-Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_module.kenlm = kenlm\n",
        "language_model_module.kenlm = kenlm\n",
        "\n",
        "decoder = build_ctcdecoder(\n",
        "    labels=vocab_list,\n",
        "    kenlm_model_path=KENLM_BINARY\n",
        ")\n"
      ],
      "metadata": {
        "id": "HRt1M8BUjBaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Line segmentation (your projection method)\n",
        "# ============================================\n",
        "def segment_lines_projection(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
        "    _, binary = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    kernel = np.ones((2, 50), np.uint8)\n",
        "    closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    hist = np.sum(closed, axis=1)\n",
        "    threshold = np.max(hist) * 0.1\n",
        "\n",
        "    lines = []\n",
        "    in_line = False\n",
        "    start = 0\n",
        "    min_line_height = 10\n",
        "    for y, val in enumerate(hist):\n",
        "        if val > threshold and not in_line:\n",
        "            in_line = True\n",
        "            start = y\n",
        "        elif val <= threshold and in_line:\n",
        "            in_line = False\n",
        "            end = y\n",
        "            if end - start >= min_line_height:\n",
        "                lines.append((start, end))\n",
        "\n",
        "    line_imgs = []\n",
        "    for i, (y1, y2) in enumerate(lines):\n",
        "        y1 = max(y1 - 10, 0)\n",
        "        y2 = min(y2 + 10, img.shape[0])\n",
        "        cropped = img[y1:y2, :]\n",
        "        pil_img = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
        "        line_imgs.append(pil_img)\n",
        "\n",
        "    return line_imgs"
      ],
      "metadata": {
        "id": "hprO32hujE5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_HEIGHT, IMG_WIDTH = 64, 800\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = img.convert(\"L\")\n",
        "    img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "    img = np.array(img, dtype=np.float32) / 255.0\n",
        "    img = np.expand_dims(img, axis=(0, -1))\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "NJxEEJHajTbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_greedy(pred):\n",
        "    pred = np.squeeze(pred)\n",
        "    best_path = np.argmax(pred, axis=-1)\n",
        "    return \"\".join([vocab_list[i] for i in best_path if i < len(vocab_list)])\n",
        "\n",
        "def decode_with_kenlm(pred):\n",
        "    pred = np.squeeze(pred)\n",
        "    return decoder.decode(pred)\n"
      ],
      "metadata": {
        "id": "sluuwJ46jWeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- inject kenlm into pyctcdecode modules (required) ---\n",
        "decoder_module.kenlm = kenlm\n",
        "language_model_module.kenlm = kenlm\n",
        "print(\"âœ… KenLM successfully linked to pyctcdecode\")\n",
        "\n",
        "# ============================================\n",
        "# Paths â€” update these to your actual Drive paths if needed\n",
        "# ============================================\n",
        "MODEL_DIR = \"/content/drive/MyDrive/htr_final_model_20251110_110833\"   # update if different\n",
        "KENLM_ARPA = \"/content/drive/MyDrive/iam_lm_5gram.arpa\"                 # your 5-gram ARPA\n",
        "\n",
        "# ============================================\n",
        "# Load your HTR model + vocab\n",
        "# ============================================\n",
        "print(\"ðŸ”„ Loading model and vocab...\")\n",
        "model = keras.models.load_model(os.path.join(MODEL_DIR, \"htr_model.keras\"), compile=False)\n",
        "vocab_list = joblib.load(os.path.join(MODEL_DIR, \"vocab_list.pkl\"))\n",
        "print(f\"âœ… Model loaded! Vocab size: {len(vocab_list)}\")\n",
        "\n",
        "# ============================================\n",
        "# Build KenLM decoder (5-gram)\n",
        "# ============================================\n",
        "print(\"ðŸ”„ Building 5-gram KenLM decoder...\")\n",
        "decoder = build_ctcdecoder(labels=vocab_list, kenlm_model_path=KENLM_ARPA)\n",
        "print(\"âœ… 5-gram KenLM decoder ready!\")\n",
        "\n",
        "# ============================================\n",
        "# Preprocessing â€” keep same size as model expects\n",
        "# ============================================\n",
        "IMG_HEIGHT, IMG_WIDTH = 64, 800   # IMPORTANT: width=800, height=64 matches model\n",
        "\n",
        "def preprocess_image(img: Image.Image):\n",
        "    # img is PIL.Image\n",
        "    img = img.convert(\"L\")\n",
        "    img = img.resize((IMG_WIDTH, IMG_HEIGHT))   # (800, 64)\n",
        "    arr = np.array(img, dtype=np.float32) / 255.0\n",
        "    arr = np.expand_dims(arr, axis=-1)          # (64,800,1)\n",
        "    arr = np.expand_dims(arr, axis=0)           # (1,64,800,1)\n",
        "    return arr\n",
        "\n",
        "# ============================================\n",
        "# Decoding helpers\n",
        "# ============================================\n",
        "def decode_greedy(pred):\n",
        "    pred = np.squeeze(pred)                     # (timesteps, vocab_size)\n",
        "    best_path = np.argmax(pred, axis=-1)\n",
        "    # collapse repeats & remove blank if using blank index â€” but your original code did simple mapping\n",
        "    # We'll do basic collapsing of repeats and remove indices >= len(vocab)\n",
        "    chars = []\n",
        "    prev = None\n",
        "    for idx in best_path:\n",
        "        if idx == prev:\n",
        "            continue\n",
        "        prev = idx\n",
        "        if idx < len(vocab_list):\n",
        "            chars.append(vocab_list[idx])\n",
        "    return ''.join(chars)\n",
        "\n",
        "def decode_with_kenlm(pred):\n",
        "    pred = np.squeeze(pred)\n",
        "    # pyctcdecode expects logits or probabilities per timestep; we pass pred as-is\n",
        "    return decoder.decode(pred)\n",
        "\n",
        "# ============================================\n",
        "# Gradio prediction function (matches your original UI)\n",
        "# ============================================\n",
        "def recognize_text(uploaded_image, ground_truth):\n",
        "    if uploaded_image is None:\n",
        "        return [], \"\", \"\", \"âš ï¸ Please upload a handwriting image.\"\n",
        "\n",
        "    # Save temporary image file so projection uses OpenCV\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp:\n",
        "        uploaded_image.save(tmp.name)\n",
        "        image_path = tmp.name\n",
        "\n",
        "    # Segment lines\n",
        "    line_images = segment_lines_projection(image_path)\n",
        "    if not line_images:\n",
        "        return [], \"\", \"\", \"âŒ No lines detected. Try a clearer image.\"\n",
        "\n",
        "    results = []\n",
        "    greedy_lines = []\n",
        "    lm_lines = []\n",
        "\n",
        "    for line_img in line_images:\n",
        "        img_input = preprocess_image(line_img)          # shape (1,64,800,1)\n",
        "        preds = model.predict(img_input, verbose=0)     # model output shape e.g. (1, T, V)\n",
        "\n",
        "        greedy_text = decode_greedy(preds)\n",
        "        lm_text = decode_with_kenlm(preds)\n",
        "\n",
        "        # show both on gallery\n",
        "        results.append((line_img, f\"ðŸ§¾ Greedy: {greedy_text}\\nðŸ“– LM: {lm_text}\"))\n",
        "        greedy_lines.append(greedy_text)\n",
        "        lm_lines.append(lm_text)\n",
        "\n",
        "    greedy_final = \"\\n\".join(greedy_lines)\n",
        "    lm_final = \"\\n\".join(lm_lines)\n",
        "\n",
        "    if isinstance(ground_truth, str) and ground_truth.strip():\n",
        "        return (\n",
        "            results,\n",
        "            greedy_final,\n",
        "            lm_final,\n",
        "            f\"WER (Greedy): {wer(ground_truth, greedy_final):.2%} | CER: {cer(ground_truth, greedy_final):.2%}\\n\"\n",
        "            f\"WER (LM): {wer(ground_truth, lm_final):.2%} | CER: {cer(ground_truth, lm_final):.2%}\"\n",
        "        )\n",
        "    else:\n",
        "        return results, greedy_final, lm_final, \"â„¹ï¸ No ground truth provided.\"\n",
        "\n",
        "# ============================================\n",
        "# Gradio UI (matches your earlier design)\n",
        "# ============================================\n",
        "with gr.Blocks(title=\"HTR + 5-Gram KenLM Handwriting Recognition\") as interface:\n",
        "    gr.Markdown(\"## ðŸ“ Handwritten Text Recognition (Greedy vs 5-gram KenLM)\")\n",
        "    gr.Markdown(\"Upload a handwritten image. The system will segment lines and show both raw (CTC greedy) and LM-enhanced predictions.\")\n",
        "\n",
        "    upload = gr.Image(type=\"pil\", label=\"ðŸ“¤ Upload Handwritten Image\")\n",
        "    ground_truth_input = gr.Textbox(label=\"âœ… Ground Truth (optional for accuracy)\", lines=4)\n",
        "    recognize_btn = gr.Button(\"ðŸ§  Recognize Text\")\n",
        "\n",
        "    gallery = gr.Gallery(label=\"ðŸ“¸ Line Predictions\", columns=1, preview=True)\n",
        "    greedy_output = gr.Textbox(label=\"ðŸ”¹ Greedy Decode (No LM)\")\n",
        "    lm_output = gr.Textbox(label=\"ðŸ”¹ 5-gram KenLM Decode (With LM)\")\n",
        "    accuracy_output = gr.Textbox(label=\"ðŸ“Š Accuracy Comparison\")\n",
        "\n",
        "    recognize_btn.click(\n",
        "        fn=recognize_text,\n",
        "        inputs=[upload, ground_truth_input],\n",
        "        outputs=[gallery, greedy_output, lm_output, accuracy_output]\n",
        "    )\n",
        "\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "A-yhrxXtkJc6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}